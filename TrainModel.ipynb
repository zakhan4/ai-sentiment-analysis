{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcHaYbHAI4cK"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from scipy import stats\n",
    "import tflearn\n",
    "from sklearn.manifold import TSNE\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras import initializers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionLayer import AttLayer\n",
    "\n",
    "#Here we add layers to our model    \n",
    "def get_blstm_atten(inp_dim, vocab_size, num_classes, learn_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, EMBED_SIZE, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(EMBED_SIZE, return_sequences=True)))\n",
    "    model.add(AttLayer())\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    adam = optimizers.Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNwzlhBGI4cR"
   },
   "outputs": [],
   "source": [
    "# Loads the already cleaned and prepared dataset from the files\n",
    "def load_cleaned_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = [] \n",
    "    for i in range(len(data)):\n",
    "        x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def load_csv_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pd.read_csv(filename,index_col=0)\n",
    "    x_text = [str(x) for x in data['text']]\n",
    "    labels = data['label']\n",
    "    return x_text,labels\n",
    "\n",
    "# Gets the filenames for a particular kind of dataset\n",
    "def get_filename(dataset):\n",
    "    global NUM_CLASSES\n",
    "    if(dataset==\"twitter\"):\n",
    "        NUM_CLASSES = 3\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "    elif(dataset==\"formspring\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "    elif(dataset==\"wiki\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "    elif(dataset==\"sentiment140\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/sentiment140.csv\" \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhoqYYy0I4cZ"
   },
   "outputs": [],
   "source": [
    "def evaluate_trained_model(model, testX, testY):\n",
    "    predictions = model.predict(testX)\n",
    "    y_pred  = np.argmax(predictions, 1)\n",
    "    y_true = np.argmax(testY, 1)\n",
    "    precisionValue = metrics.precision_score(y_true, y_pred, average=None)\n",
    "    recallValue = metrics.recall_score(y_true, y_pred, average=None)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    print(\"Precision: \" + str(precisionValue) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recallValue) + \"\\n\")\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    return precisionValue, recallValue, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmL7pTh2I4cg"
   },
   "outputs": [],
   "source": [
    "def get_train_test_split(data, x_text, labels):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=42, test_size=0.10)\n",
    "    \n",
    "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "    if(data != \"twitter\"):\n",
    "        max_document_length = int(np.percentile(post_length, 95))\n",
    "    else:\n",
    "        max_document_length = max(post_length)\n",
    "    print(\"Document length : \" + str(max_document_length))\n",
    "    \n",
    "    # Maps documents to vectors of words ids of length max_document_length by padding or clipping numbers\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "    vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "    \n",
    "    trainY = np.asarray(Y_train)\n",
    "    testY = np.asarray(Y_test)\n",
    "    \n",
    "    # Just make each vector into equal length of max_document_length\n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "    \n",
    "    # Convert numbers to binary matrix\n",
    "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
    "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
    "    \n",
    "    data_dict = {\n",
    "        \"data\": data,\n",
    "        \"trainX\" : trainX,\n",
    "        \"trainY\" : trainY,\n",
    "        \"testX\" : testX,\n",
    "        \"testY\" : testY,\n",
    "        \"vocab_processor\" : vocab_processor\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ud46b4DGI4cj"
   },
   "outputs": [],
   "source": [
    "def return_data(data_dict):\n",
    "    return data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKcJ6Ym5I4co"
   },
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVNWpanuI4cs"
   },
   "outputs": [],
   "source": [
    "def train_evaluate(data_dict, dump_embeddings=False):\n",
    "\n",
    "    data, trainX, trainY, testX, testY, vocab_processor = return_data(data_dict)\n",
    "    \n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "    \n",
    "    print(\"Running Model...\")\n",
    "    model = get_blstm_atten(trainX.shape[1], vocab_size, NUM_CLASSES, LEARN_RATE)\n",
    "\n",
    "    initial_weights = model.get_weights()\n",
    "    shuffle_weights(model, initial_weights)\n",
    "    \n",
    "    model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, verbose=1)\n",
    "            \n",
    "    if (dump_embeddings==True):\n",
    "        embed = model.layers[0].get_weights()[0]\n",
    "    \n",
    "        embed_filename = output_folder_name + data + \".pkl\"\n",
    "        embed.dump(embed_filename)\n",
    "        \n",
    "        vbp_filename = output_folder_name + data + \"_vbp.pkl\"\n",
    "        vocab_processor.save(vbp_filename)\n",
    "            \n",
    "        model_filename = output_folder_name + data + \".model\"\n",
    "        model.save(model_filename)\n",
    "    \n",
    "    return  evaluate_trained_model(model, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuwTMrseI4dJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Does Over Sampling and removing punctuation from the text\n",
    "def get_data(data):\n",
    "    if(data == \"sentiment140\"):\n",
    "        x_text2, labels2 = load_csv_data(get_filename(data)) \n",
    "        x_text, labels = load_cleaned_data(get_filename(\"wiki\"))\n",
    "    else:\n",
    "        x_text, labels = load_cleaned_data(get_filename(data)) \n",
    "    \n",
    "    if(data==\"twitter\"):\n",
    "        NUM_CLASSES = 3\n",
    "        dict1 = {'racism':2,'sexism':1,'none':0}\n",
    "        labels = [dict1[b] for b in labels]\n",
    "        \n",
    "        racism = [i for i in range(len(labels)) if labels[i]==2]\n",
    "        sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in racism]*(OVERSAMPLING_RATE-1)+ [x_text[x] for x in sexism]*(OVERSAMPLING_RATE-1)\n",
    "        labels = labels + [2 for i in range(len(racism))]*(OVERSAMPLING_RATE-1) + [1 for i in range(len(sexism))]*(OVERSAMPLING_RATE-1)\n",
    "    \n",
    "    else:  \n",
    "        NUM_CLASSES = 2\n",
    "        bully = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in bully]*(OVERSAMPLING_RATE-1)\n",
    "        labels = list(labels) + [1 for i in range(len(bully))]*(OVERSAMPLING_RATE-1)\n",
    "        \n",
    "    if(data == \"sentiment140\"):\n",
    "        x_text.extend(x_text2)\n",
    "        labels.extend(labels2)\n",
    "\n",
    "    print(\"Counter after oversampling\")\n",
    "    print(Counter(labels))\n",
    "        \n",
    "    return x_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NRFzyypI4dO"
   },
   "outputs": [],
   "source": [
    "def train_model(data):    \n",
    "    x_text, labels = get_data(data)\n",
    "    data_dict = get_train_test_split(data,  x_text, labels)\n",
    "    train_evaluate(data_dict, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDSDmdYaI4dM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data/formspring_data.pkl\n",
      "Counter after oversampling\n",
      "Counter({0: 11997, 1: 2328})\n",
      "Document length : 62\n",
      "Vocabulary Size: 7190\n",
      "Running Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 62, 50)            359500    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 62, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 62, 100)           40400     \n",
      "_________________________________________________________________\n",
      "att_layer_6 (AttLayer)       (None, 100)               100       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 400,202\n",
      "Trainable params: 400,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "12892/12892 [==============================] - 32s 3ms/step - loss: 0.4756 - acc: 0.8047\n",
      "Epoch 2/10\n",
      "12892/12892 [==============================] - 35s 3ms/step - loss: 0.2845 - acc: 0.8861\n",
      "Epoch 3/10\n",
      "  512/12892 [>.............................] - ETA: 41s - loss: 0.1871 - acc: 0.9414"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-d0d7b9221fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mDATA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"formspring\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-91-8a6917eeb942>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-8d90802900cf>\u001b[0m in \u001b[0;36mtrain_evaluate\u001b[1;34m(data_dict, dump_embeddings)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mshuffle_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdump_embeddings\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\theano\\ifelse.py\u001b[0m in \u001b[0;36mthunk\u001b[1;34m()\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "MAX_FEATURES = 2\n",
    "NUM_CLASSES = None\n",
    "DROPOUT = 0.25\n",
    "LEARN_RATE = 0.01\n",
    "EMBED_SIZE = 50\n",
    "# param OVERSAMPLING_RATE is the multiple by which bullying or sexist/racist data is increased\n",
    "OVERSAMPLING_RATE = 3\n",
    "output_folder_name = \"results/\"\n",
    "#Selects the dataset to use to train\n",
    "#Can select from [formspring, wiki, twitter, sentiment104]\n",
    "DATA = \"formspring\"\n",
    "\n",
    "train_model(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNNs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
